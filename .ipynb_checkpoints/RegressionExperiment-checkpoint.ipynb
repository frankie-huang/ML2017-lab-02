{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-frankie-桌面-机器学习实验-frankie实验二-__ipython-input__.get_data...\n",
      "get_data('a9a')\n",
      "_________________________________________________________get_data - 0.4s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-frankie-桌面-机器学习实验-frankie实验二-__ipython-input__.get_data...\n",
      "get_data('a9a.t')\n",
      "_________________________________________________________get_data - 0.2s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(filename):\n",
    "    data = load_svmlight_file(filename)\n",
    "    return data[0], data[1]\n",
    "\n",
    "x_train, y_train = get_data(\"a9a\")\n",
    "x_test, y_test = get_data(\"a9a.t\")\n",
    "\n",
    "# 32561 x 123\n",
    "x_train = x_train.toarray()\n",
    "x_train = torch.from_numpy(x_train).type(dtype)\n",
    "\n",
    "bias = torch.ones(x_train.size(0), 1)\n",
    "x_train = torch.cat((x_train, bias), 1)\n",
    "\n",
    "# 32561 x 1\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "y_train = torch.from_numpy(y_train).type(dtype)\n",
    "\n",
    "# 16281 x 122\n",
    "x_test = x_test.toarray()\n",
    "x_test = torch.from_numpy(x_test).type(dtype)\n",
    "\n",
    "# 16281 x 1\n",
    "y_test = np.array(y_test).reshape(-1,1)\n",
    "y_test = torch.from_numpy(y_test).type(dtype)\n",
    "\n",
    "dim = max(x_train.size(1)-1, x_test.size(1))\n",
    "x_test = torch.cat((x_test, torch.zeros(x_test.size(0), dim - x_test.size(1))), 1) # 补0\n",
    "\n",
    "bias = torch.ones(x_test.size(0), 1)\n",
    "x_test = torch.cat((x_test, bias), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.W = []        \n",
    "        # 模型参数全零初始化\n",
    "        self.W.append(torch.zeros(1, x_train.size(1)))\n",
    "    \n",
    "    def get_W(self):\n",
    "        return self.W[-1]\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        margin = y * torch.mm(x, self.W[-1].t())\n",
    "        logistic = 1.0 / (1 + torch.exp(-margin))\n",
    "        log_logistic = torch.log(logistic)\n",
    "        loss = - torch.sum(log_logistic) / x.size(0)\n",
    "        return loss\n",
    "\n",
    "    def grad(self, W, x, y):\n",
    "        margin = y * torch.mm(x, W.t())\n",
    "        logistic = 1.0 / (1 + torch.exp(-margin))\n",
    "        dW = - torch.sum(torch.mul(1 - logistic, y * x), dim = 0) / x.size(0)\n",
    "        dW = dW.numpy().reshape(-1,1)\n",
    "        dW = torch.from_numpy(dW).type(dtype)\n",
    "        return dW.t()\n",
    "\n",
    "    def predict(self, x, threshold = 0.0):\n",
    "        h = torch.mm(x, self.W[-1].t())\n",
    "        y_pred = torch.ones((x.size(0),1))\n",
    "        y_pred[h < threshold] = -1\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, optimizer, train_params, optimizer_params):\n",
    "        # track the loss history\n",
    "        loss_validation_history = []\n",
    "        iteras = []\n",
    "            \n",
    "        # track the best accuracy and loss respectly\n",
    "        best_acc = 0.0\n",
    "        best_it = 0\n",
    "        lowest_loss  = 1e9\n",
    "        lowest_it = 0\n",
    "        \n",
    "        # get the train parameters\n",
    "        num_iters = train_params.setdefault('num_iters', 100)\n",
    "        batch_size = train_params.setdefault('batch_size', 10)\n",
    "        threshold = train_params.setdefault('threshold', 0.0)\n",
    "        verbose = train_params.setdefault('verbose', True)\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            batch_mask = np.random.choice(x_train.size(0), batch_size)\n",
    "            batch_mask = torch.from_numpy(batch_mask)\n",
    "            x_batch = x_train[batch_mask]\n",
    "            y_batch = y_train[batch_mask]     \n",
    "            \n",
    "            grad = lambda W: self.grad(W, x_batch, y_batch)\n",
    "            \n",
    "            # use optimizer to upate the weight\n",
    "            new_W, optimizer_params = optimizer(self.W[-1], grad, optimizer_params)\n",
    "            self.W.append(new_W)\n",
    "            \n",
    "            # calculate the loss on validataion data\n",
    "            loss_vali = self.loss(x_test, y_test)\n",
    "            loss_validation_history.append(loss_vali)\n",
    "            \n",
    "            # Calculate the accuracy on validation data\n",
    "            y_pred = self.predict(x_test, threshold)\n",
    "            acc = torch.mean((y_pred == y_test).type(dtype))\n",
    "            \n",
    "            # Update the best result\n",
    "            if best_acc < acc:\n",
    "                best_acc = acc\n",
    "                best_it = it + 1\n",
    "            if lowest_loss > loss_vali:\n",
    "                lowest_loss = loss_vali\n",
    "                lowest_it = it + 1\n",
    "            if verbose:\n",
    "                print('iteration %d / %d: validation_loss %f, accuracy %f' % (it + 1, num_iters , loss_vali, acc))\n",
    "        print(\"Best result:  iteration %d,  accuracy %f\" %(best_it, best_acc))\n",
    "        print(\"Lowest loss:  iteration %d,  loss %f\" %(lowest_it, lowest_loss))\n",
    "        return loss_validation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG(W, grad, optimizer_params):\n",
    "    v = optimizer_params.setdefault(\"v\", torch.zeros(W.size()))\n",
    "    mu = optimizer_params.setdefault(\"mu\", 0.9)\n",
    "    lr = optimizer_params.setdefault(\"lr\", 1e-4)\n",
    "    \n",
    "    # to calculate ahead grad\n",
    "    dW = grad(W + mu*v)\n",
    "\n",
    "    # calculate the momentum\n",
    "    v = mu*v - lr * dW\n",
    "    \n",
    "    # update the self.W\n",
    "    next_W = W + v\n",
    "    \n",
    "    # updata the params\n",
    "    optimizer_params['v'] = v\n",
    "    return next_W, optimizer_params\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_params = {\n",
    "    'num_iters': 20000,\n",
    "    'batch_size': 10,\n",
    "    'threshold': 0.0,\n",
    "    'verbose': False\n",
    "}\n",
    "optimizer_params = {\n",
    "    'mu': 0.9,\n",
    "    'lr': 1e-3\n",
    "}\n",
    "\n",
    "loss_vali_NAG = model.train(NAG, train_params, optimizer_params)\n",
    "# plot the loss history\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) #set default size\n",
    "plt.plot(loss_vali_NAG, label = \"NAG\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSProp(W, grad, optimizer_params):\n",
    "    lr = optimizer_params.setdefault(\"lr\", 1e-4)\n",
    "    decay_rate = optimizer_params.setdefault(\"decay_rate\", 0.9)\n",
    "    cache = optimizer_params.setdefault(\"cache\", torch.zeros(W.size()))\n",
    "    eps = optimizer_params.setdefault(\"eps\", 1e-7)\n",
    "    \n",
    "    dW = grad(W)\n",
    "    cache = decay_rate*cache + (1 - decay_rate) * dW**2\n",
    "    next_W = W - lr * dW / (torch.sqrt(cache) + eps)\n",
    "    optimizer_params['cache'] = cache\n",
    "    return next_W, optimizer_params\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_params = {\n",
    "    'num_iters': 6000,\n",
    "    'batch_size': 10,\n",
    "    'threshold': 0.0,\n",
    "    'verbose': False\n",
    "}\n",
    "optimizer_params = {\n",
    "    'lr': 1e-3,\n",
    "    'decay_rate':0.9\n",
    "}\n",
    "\n",
    "loss_vali_RMSProp = model.train(RMSProp, train_params, optimizer_params)\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(loss_vali_RMSProp,label = \"RMSProp\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AdaDelta(W, grad, optimizer_params):\n",
    "    decay_rate = optimizer_params.setdefault(\"decay_rate\", 0.9)\n",
    "    cache_grad = optimizer_params.setdefault(\"cache_grad\", torch.zeros(W.size()))\n",
    "    cache_step = optimizer_params.setdefault(\"cache_step\", torch.zeros(W.size()))\n",
    "    eps = optimizer_params.setdefault(\"eps\", 1e-6)\n",
    "    \n",
    "    # to calculate ahead grad\n",
    "    dW = grad(W)\n",
    "            \n",
    "    # keeps track of per-parameter sum of squared gradients.\n",
    "    cache_grad = decay_rate*cache_grad + (1 - decay_rate) * (dW**2)\n",
    "    \n",
    "    # calculate the step based on cache_step and cache_grad\n",
    "    step = - (torch.sqrt(cache_step) + eps) * dW / (torch.sqrt(cache_grad) + eps)\n",
    "    \n",
    "    # update the self.W\n",
    "    next_W = W + step\n",
    "    \n",
    "    # keeps track of step\n",
    "    cache_step = decay_rate*cache_step + (1 - decay_rate) * (step**2)    \n",
    "    \n",
    "    # updata the params\n",
    "    optimizer_params['cache_step'] = cache_step\n",
    "    optimizer_params['cache_grad'] = cache_grad\n",
    "    \n",
    "    return next_W, optimizer_params\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_params = {\n",
    "    'num_iters': 1000,\n",
    "    'batch_size': 10,\n",
    "    'threshold': 0.0,\n",
    "    'verbose': False\n",
    "}\n",
    "optimizer_params = {\n",
    "    'decay_rate': 0.95,\n",
    "    'eps': 0.001\n",
    "}\n",
    "\n",
    "loss_vali_AdaDelta = model.train(AdaDelta, train_params, optimizer_params)\n",
    "# plot the loss history\n",
    "plt.plot(loss_vali_AdaDelta,label = \"AdaDelta\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Adam(W, grad, optimizer_params):\n",
    "    lr = optimizer_params.setdefault(\"lr\", 1e-4)\n",
    "    beta1 = optimizer_params.setdefault(\"beta1\", 0.9)\n",
    "    beta2 = optimizer_params.setdefault(\"beta2\", 0.999)\n",
    "    moment = optimizer_params.setdefault(\"moment\", torch.zeros(W.size()))\n",
    "    cache_grad = optimizer_params.setdefault(\"cache_grad\", torch.zeros(W.size()))\n",
    "    it = optimizer_params.setdefault(\"it\",1)\n",
    "    eps = optimizer_params.setdefault(\"eps\", 1e-6)\n",
    "    \n",
    "    dW = grad(W)\n",
    "    # keep the moment\n",
    "    moment = beta1*moment + (1-beta1)*dW\n",
    "\n",
    "    moment_scale = moment / (1 - beta1**it)\n",
    "\n",
    "    # keep the squared gradients\n",
    "    cache_grad = beta2 * cache_grad + (1 - beta2) * (dW ** 2)\n",
    "    cache_grad_scale = cache_grad / (1- beta2**it) # \n",
    "\n",
    "    # update self.W\n",
    "    next_W = W - lr * moment_scale / (torch.sqrt(cache_grad_scale) + eps)    \n",
    "    \n",
    "    # updata the params\n",
    "    optimizer_params['moment'] = moment\n",
    "    optimizer_params['cache_grad'] = cache_grad\n",
    "    optimizer_params['it'] = it + 1\n",
    "                                     \n",
    "    return next_W, optimizer_params\n",
    "\n",
    "model = LogisticRegression()\n",
    "train_params = {\n",
    "    'num_iters': 12000,\n",
    "    'batch_size': 10,\n",
    "    'threshold': 0.0,\n",
    "    'verbose': False\n",
    "}\n",
    "optimizer_params = {\n",
    "    'lr': 1e-3,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "}\n",
    "\n",
    "loss_vali_Adam = model.train(Adam, train_params, optimizer_params)\n",
    "# plot the loss history\n",
    "plt.plot(loss_vali_Adam,label = \"Adam\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_vali_NAG,label = \"NAG\")\n",
    "plt.plot(loss_vali_RMSProp,label = \"RMSProp\")\n",
    "plt.plot(loss_vali_AdaDelta,label = \"AdaDelta\")\n",
    "plt.plot(loss_vali_Adam,label = \"Adam\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss history')\n",
    "plt.legend(loc = 'best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
